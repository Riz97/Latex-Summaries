\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsfonts}


\title{Computational Vision}
\author{Riccardo Caprile}
\date{March 2022}

\begin{document}

\maketitle

\section{Image Processing}

\subsection{Describing the the whole image}

\textbf{Histograms} can be used as a way of obtaining an overall description of the image content. It is easy to compute , robust to geometrical variations and to scale changes.
But you loose spatial information such as pixels' position , i know the quantity of a value but i don't know where it is.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{1.PNG}
  \end{subfigure}
\end{figure}

\subsection{Image Filters}

A basic tool in image processing used for a variety of tasks , included noise reduction and signal enhancement.

We will consider linear filters in the space domain.


\vspace{30mm}

\subsubsection{Linear filtering in space - Convolution}

In linear cases , filtering corresponds to applying to a signal f a convolution operator.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{2.PNG}
  \end{subfigure}
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=1\textwidth]{3.PNG}
     \end{subfigure}
\end{figure}

In the first example , along y i should be see 0 whereas along x some changes. It is the opposite for the example after. There is a change in intensity level only in the horizontal direction in the first case and no change in the y direction.
Gradient magnitude represents the strength of the change in the intensity level of the image. Higher Gradient magnitude , stronger the change in the image intensity.

\subsubsection{The effect noise}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{4.PNG}
  \end{subfigure}
\end{figure}

With the noisy signal derivative , it clearly loose the significant change of frequency that we have in the original signal.

\vspace{40mm}

\subsubsection{Smoothing}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{5.PNG}
  \end{subfigure}
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=1\textwidth]{6.PNG}
     \end{subfigure}
\end{figure}

Using directly the derivative of the Kernel will save us one operation.

\subsection{Local features : edges}

Edges are pixels at which the image values undergo a sharp variation.

Edge detection : given an image locate edges most likely to be generated by scene elements and not by noise.

\subsubsection{Edges or objects boundaries?}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{7.PNG}
  \end{subfigure}
\end{figure}

Edges and boundaries are two different concepts (texture of the animal is not a boundary but could be an edge).

\subsubsection{Corners : Good features to match}

We observe how patches with gradients in at least two different orientations are the easier to localize.
This can be formalized by analysing a simple matching criterium.

Take a portion of an image , and i look to the similarities between that portion and its shifted version.
I($x_i$) is the portion of the image centered in that point.

Low autocorrelation : the two portion are similar

The \textbf{gradient} can be defined as the change in the direction of the intensity level of an image.


These intutitions can be formalized by looking at the simplest possible matching criterion for comparing two image patches (their summed sqaure difference).

When performing feature detection , we do not know which other image locations the feature will end up being matched against. Therefore , we can only compute how stable this metric is with respect to small variations in position Delta u , comparing an image patch against itself


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=1\linewidth]{8.PNG}
  \end{subfigure}
\end{figure}

It is interesting to see in the image c lower values are near the edge and higher when we move away from it. In the image b , there is a strong minimum indicating that it can be well localized.

\vspace{60mm}

\subsection{Local features : corners}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.9\linewidth}
    \includegraphics[width=1\linewidth]{10.PNG}
  \end{subfigure}
\end{figure}

How can we find image locations where we can reliably find correspondences with other images? As you may notice , textureless patches are nearly impossible to localize.Patches with a large contrast changes (gradients) are easier to localize. Patches with gradients in at least two significantly different orientations are the easiest to localize.



Corners correspond to points where the image gradient varies in at least two directions.

They can be detected by computing and analyzing the autocorrleation matrix A of a patch around each point. ( Given an image point q we consider a neighborhood Nq and compute its autocorrelation matrix)
Autocorrelation consider a patch and the entire image.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.9\linewidth}
    \includegraphics[width=1\linewidth]{9.PNG}
  \end{subfigure}
\end{figure}


\section{Image Matching}

\subsection{Introduction}

The concept of estimating the similarity between image pairs is very broad and is usually guided by the application.

Similarity is usually estimated with an image matching process where we try to match one image (or parts of it) with another.

\begin{itemize}
    \item \textbf{Global matching} : Compute a global descriptor for each image (eg, brightness/color mean and variance , color histograms ) and estimate the similarity between descriptors.
    \item \textbf{Local matching} : Extract local features and solve a feature correspondence problem
\end{itemize}

\subsection{Local matching as a correspondence problem}

It involves two decisions : 

\begin{enumerate}
    \item Which image elements to match (all the pixels from an image (\textbf{dense correspondences}) , or a subset of pixels meeting some requirements (\textbf{sparse correspondences})
    \item Which feature description + similarity to adopt
\end{enumerate}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{11.PNG}
  \end{subfigure}
\end{figure}

\subsection{Our goal : local matching}

Match elements in different images. Objects may appear at different scales , rotation or viewpoint.

Pipeline : 

\begin{enumerate}
    \item \textbf{Feature Detection} : Find interest points on each image
    \item \textbf{Feature Description} : Compute a vector description for each point
    \item \textbf{Feature Matching}
\end{enumerate}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{12.PNG}
  \end{subfigure}
\end{figure}

We simply obtained the image by translating the camera.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{13.PNG}
  \end{subfigure}
\end{figure}

Images taken from different devices and part of the day , because of the light we need some kind of normalization.

\subsection{Local Feature Matching}

If image pairs which are similar enough , then local features undergo a quasi translation transformation.

Interest points may be \textbf{corners}.

\textbf{Image patches} are an appropriate feature description.

In the case of scale , rotation and more severe viewpoint changes we may need \textbf{scale invariant} interest points and better feature descriptors.

\vspace{50mm}

\subsubsection{Local Feature Matching : Detection Problems}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=1\linewidth]{14.PNG}
  \end{subfigure}
\end{figure}

\subsection{Scale invariant Feature detectors}

In many situations , detecting features at the finest stable scale possible may not be appropriate. For example , when matching images with little high-frequency detail such as clouds.
One solution to the problem is to extract features at a variety of scales ,  for example by performing the same operations at multiple resolutions in a pyramid and then matching features at the same level.

In the scaled image we need a larger square because we need the same amount of information.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{15.PNG}
  \end{subfigure}
\end{figure}

In the next slide level5 is a the image filtered 5 times. In the first image we want to take a point in a very small neighbourhood. Going on with the other filtered images we want to have more general details and similarities. Standard deviation of the Gaussian grows and so grows the number of points in the neighbourhod  we are considering.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=1\linewidth]{16.PNG}
  \end{subfigure}
\end{figure}

\vspace{60mm}


However , for most object recognition applications , the scale of the object in the image is unknown. 
Instead of extracting features at many different scales and then matching all of them , it is more efficient to extract features that are stable in both location and scale.

Lindeberg proposed using extrema (max and min) in the Laplacian of Gaussian (LoG) function as interest point locations. It is the filter used before that allows you to implement blob detection.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{17.PNG}
  \end{subfigure}
\end{figure}


\vspace{70mm}

\subsection{Blob-like features}

Another type of local key point quite naturally associated with the concept of scale.

\textbf{Blobs} : uniform areas surrounded by a sharp variation of the signal.

Blob enhancement may be carried out by looking for extrema of the image Laplacian.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{18.PNG}
  \end{subfigure}
\end{figure}


We define the \textbf{characteristics scale} as the scale that produces peak of Laplacian response.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{19.PNG}
  \end{subfigure}
\end{figure}

\subsubsection{Scale-space blob detector : Example}


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{20.PNG}
  \end{subfigure}
\end{figure}

\subsection{SIFT Explanation in Details}

\subsubsection{Scale-Space Peak Selection}

\textbf{Scale space} : Real world objects are meaningful only at a certain scale. You might see a sugar cube perfectly on a table. But if looking at the entire milky way, then it simply does not exist. This multi-scale nature of objects is quite common in nature. And a scale space attempts to replicate this concept on digital images.

The scale space of an image is a function $L(x,y,\sigma)$ that is produced from the convolution of a Gaussian kernel(Blurring) at different scales with the input image. Scale-space is separated into octaves and the number of octaves and scale depends on the size of the original image. So we generate several octaves of the original image. Each octave’s image size is half the previous one.
G is the Gaussian Blur operator and I is an image. While x,y are the location coordinates and $\sigma$ is the “scale” parameter. Think of it as the amount of blur. Greater the value, greater the blur.



\subsubsection{DoG , Difference of Gaussian kernel}

Now we use those blurred images to generate another set of images, the Difference of Gaussians (DoG). These DoG images are great for finding out interesting keypoint in the image. The difference of Gaussian is obtained as the difference of Gaussian blurring of an image with two different $\sigma$, let it be $\sigma$ and $\sigma$. This process is done for different octaves of the image in the Gaussian Pyramid. It is represented in below image : 

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{30.PNG}
  \end{subfigure}
\end{figure}

\subsubsection{Finding keypoints}

Up till now, we have generated a scale space and used the scale space to calculate the Difference of Gaussians. Those are then used to calculate Laplacian of Gaussian approximations that are scale invariant.
One pixel in an image is compared with its 8 neighbors as well as 9 pixels in the next scale and 9 pixels in previous scales. This way, a total of 26 checks are made. If it is a local extrema, it is a potential keypoint. It basically means that keypoint is best represented in that scale.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=1\linewidth]{31.PNG}
  \end{subfigure}
\end{figure}

\subsubsection{Keypoint localization}

Key0points generated in the previous step produce a lot of keypoints. Some of them lie along an edge, or they don’t have enough contrast. In both cases, they are not as useful as features. So we get rid of them. The approach is similar to the one used in the Harris Corner Detector for removing edge features. For low contrast features, we simply check their intensities.

\subsubsection{Orientation Assignment}

Now we have legitimate keypoints. They’ve been tested to be stable. We already know the scale at which the keypoint was detected (it’s the same as the scale of the blurred image). So we have scale invariance. The next thing is to assign an orientation to each keypoint to make it rotation invariance.
A neighborhood is taken around the keypoint location depending on the scale, and the gradient magnitude and direction is calculated in that region. An orientation histogram with 36 bins covering 360 degrees is created. Let's say the gradient direction at a certain point (in the “orientation collection region”) is 18.759 degrees, then it will go into the 10–19-degree bin. And the “amount” that is added to the bin is proportional to the magnitude of the gradient at that point. Once you’ve done this for all pixels around the keypoint, the histogram will have a peak at some point.The highest peak in the histogram is taken and any peak above 80percent of it is also considered to calculate the orientation. It creates keypoints with same location and scale, but different directions. It contributes to the stability of matching.

\subsubsection{Keypoint descriptor}

At this point, each keypoint has a location, scale, orientation. Next is to compute a descriptor for the local image region about each keypoint that is highly distinctive and invariant as possible to variations such as changes in viewpoint and illumination.

To do this, a 16x16 window around the keypoint is taken. It is divided into 16 sub-blocks of 4x4 size.

For each sub-block, 8 bin orientation histogram is created.

So 4 X 4 descriptors over 16 X 16 sample array were used in practice. 4 X 4 X 8 directions give 128 bin values. It is represented as a feature vector to form keypoint descriptor. This feature vector introduces a few complications. We need to get rid of them before finalizing the fingerprint.

\subsubsection{Keypoint Matching}

Keypoints between two images are matched by identifying their nearest neighbors. But in some cases, the second closest-match may be very near to the first. It may happen due to noise or some other reasons. In that case, the ratio of closest-distance to second-closest distance is taken. If it is greater than 0.8, they are rejected. It eliminates around 90percent of false matches while discards only 5percent correct matches, as per the paper.



\subsection{DoG Features}


It can be shown that the Laplacian operator may be approximated by the difference of the image smoothed with two different Gaussian filters

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{21.PNG}
  \end{subfigure}
\end{figure}

\subsection{Feature Descriptors}

After detecting keypoint features , we must match them. We must determine which features come from corresponding locations in different images. In some cases the sum of squared difference can be used to directly compare the intensities in small patches around each feature point.
Even after compensating for these changes , the local appearance of image patches will usually still vary from image to image. How can we make image descriptors more invariant to such changes, while still preserving disciminability between different patches?

\subsubsection{Raw patches}

The simplest way to describe the neighborhood around an interest point is to write down the list of intensities to form a vector . But this is very sensitive to even small shifts , rotations.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=1\linewidth]{22.PNG}
  \end{subfigure}
\end{figure}

\subsubsection{Invariant Feature Descriptors : Scale Invariant Feature Transform SIFT}

The basic idea is to take a 16x16 square window around detected interest point ( in the case of this image is 8x8).

Compute edge orientation (angle of the gradient minus 90) for each pixel.

Throw out weak edges ( threshold gradient magnitude).

Create histogram of surviving edge orientation.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{23.PNG}
  \end{subfigure}
\end{figure}

\subsubsection{Invariant Feature Descriptors : Multiscale Oriented PatcheS Descriptors MOPS}

Take a 40x40 square window around detected feature.

Scale to 1/5 size to get 8x8 square window.

Rotate to horizontal.

Normalize the window values by subtracting the mean and dividing bt the standard deviation in the window.

\subsection{SIFT by David Lowe}

The idea is to detect interesting points such as corners or blobs , on a multi-scale image representation , and then associate with them a vectorial description which is invariant to translation , scale and rotation changes.

This idea is very robust , fast , efficient and tolerant to illumination changes.

\subsection{DoG detection and SIFT description}

Main steps : 

\begin{enumerate}
    \item Scale-space image construction
    \item Scale-space extrema detection (DoG keypoints)
    \item Accurate key point location ( sub-pixels analysis , remove low contrast and edges)
    \item Keypoint orientation assignment
    \item Keypoint descriptor
\end{enumerate}

This is a schematic representation of Lowe SIFT. (a) Gradient orientations and magnitude are computed at each pixel and weighted by a Gaussian window ( blue circle). (b) A weighted gradient orientation histogram is then computed in each subregion. The highest peak in the histogram gives the main orientation of the feature.
8 bins for the 8 different directions in the keypoint descriptor

Each local feature is represented collecting the information of the gradient around its location , considering a patch of size corresponding to its scale and rotated according to its orientation

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{24.PNG}
  \end{subfigure}
\end{figure}

\subsection{Feature Matching}

Once we have extracted features and their descriptors from two or more images, the next
step is to establish some preliminary feature matches between these images.
Determining which feature matches are reasonable to process further depends on the context
in which the matching is being performed. Say we are given two images that overlap to a
fair amount (e.g., for image stitching or for tracking objects in a video). We know that most
features in one image are likely to match the other image, although some may not match
because they are occluded or their appearance has changed too much.

\subsubsection{Similarity measures for image patches}

Let N1 and N2 be two square image patches of size WxW.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{25.PNG}
  \end{subfigure}
\end{figure}

How to measure similarity? Using Sum of squared differences or Normalized cross correlation.

\subsubsection{Similarity measures for histograms}

Given a Euclidean distance metric , the simplest matching strategy is to set a threshold (maximum distance) and to return all matches form other images within this threshold. Setting the threshold too high results in too many false positive ( incorrect matches being returned). Setting the threshold too low results in too many false negatives , too many correct matches being missed. We can evaluate the performance of a matching algorithm at a particular threshold by  first counting the number of true and false matches and match failures .

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{26.PNG}
  \end{subfigure}
\end{figure}

\subsection{Matching Strategy}

To generate candidate matches , find features with the most familiar appearance. Brute force approach : compare them all , take the closest ( or closest k , or within a tresholded distance).

To add robustness to matching , consider ratio : dist to best match / dist to second best match

If low , first match looks good.

If high , could be ambiguous match.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{27.PNG}
  \end{subfigure}
\end{figure}

\vspace{70mm}

\subsubsection{Matching through an affinity matrix}

In graph theory , an affinity matrix is similar to an adjacency matrix E but its entries take values $[0,1]$.

It describes the similarity between graph nodes and between features.

Each entry measures how similar two keypoints are.


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{28.PNG}
  \end{subfigure}
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=1\textwidth]{29.PNG}
     \end{subfigure}
\end{figure}


\section{Motion Analysis}

\subsection{Introduction}

What do we gain if we analyse videos instead of images?

We focus on motion information.

We are observing a scene with one camera acquiring a set of images "close in time".

\textbf{Image sequence} : Series of N images , or frames , acquired at discrete time instants

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{32.PNG}
  \end{subfigure}
\end{figure}

\subsubsection{Brightness Constancy assumption}

An important assumption to make when analysing image sequences is the following: \textbf{Illumination does not vary in the observation interval} , but what does this mean? 

As i'm analyzing multiple frames in my video , i'm assuming that something is changing due to motion , not to lightning.

It this case the image changes from t1 to t2 are caused by the relative motion between scene and camera.

\subsubsection{The problems of motion}

We may identify different types of questions related with motion analysis.

\begin{itemize}
    \item \textbf{Correspondence}: Which elements of a frame correspond to which element of the next frame? We have further knowledge than feature matching because we have more images to use
    \item \textbf{Reconstruction} : What was the 3D position and 3D velocity of the observed elements?
    \item \textbf{Motion segmentation} : What regions of the image plane correspond to different moving parts? In the case the camera is still , motion segmentation is called change detection. A frame is divided , for example , in 4 subregion , and every element of a subregion should move in the same direction of all the element that belong to the same subregion.
    \item \textbf{Tracking} : Estimate the trajectories of points or objects from image sequences
\end{itemize}

\vspace{70mm}

\subsection{Motion Segmentation : Change Detection}

In this case the camera is stil.

The analysis is pixel based.

Black : elements that are not moving

White : elements that are moving

$I_t$ is my image , $I_ref$ is the reference image (image of the empty room) for detecting image changes.

x,y is the position of the pixel i am considering.

Easier example : $I_t$ is a black image with a white circle in the middle , $I_ref$ is just a black image.

Why do we need absolute value ? We need values that are not minor than 0.

The threshold $\tau$ depends on the acquisition device and the acquisition conditions. It must be chosen considering a trade-off between false positives and false negatives (noise)





\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=1\linewidth]{33.PNG}
  \end{subfigure}
\end{figure}

\subsubsection{The threshold}


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{34.PNG}
  \end{subfigure}
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=1\textwidth]{35.PNG}
     \end{subfigure}
\end{figure}


\subsubsection{The reference image}

The simplest way of detecting moving objects is to compare consecutive frames. In general is not a good choice , because it is difficult to detect slow motion . Noisy localization

The reference image or , more in general the background model , is a picture of the empty scene , containing all the parts which are not moving

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{36.PNG}
  \end{subfigure}
\end{figure}


\textbf{The reference image - Version 1}

The reference image is a picture of the empty scene , containing all the parts which are not moving.

The simplest way of computing it is to average the first N frames (assuming that at the beginning the scene is stationary)

 
\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=1\linewidth]{37.PNG}
  \end{subfigure}
\end{figure}

I am assuming that the first frames are empty.

\vspace{30mm}

\textbf{The reference image - Version 2 (Running Average)}


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=1\linewidth]{38.PNG}
  \end{subfigure}
\end{figure}

Alpha is a learning factor. What we are doing here , is basically refreshing the background.

This method it is quite robust to moving objects in the scene. It incorporates smooth stable changes and it is simple and efficient.

The problem is that it does not deal with repetitive motion.

\subsection{Correspondence : Optical Flow}

\textbf{Correspondence} : Which elements of a frame correspond to which elements of the next frame?

We take advantage of the high similarity between adjacent frames.

I want to understand how things are moving in an image plane.

Correlation-based approaches and Gradient-based approaches

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{39.PNG}
  \end{subfigure}
\end{figure}

\subsubsection{Motion analysis as a correspondence problem}

In the case of motion , correspondence may also be seen as a problem of estimating the apparent motion of the brightness pattern ( the so called optical flow , which is the estimation of the apparent motion)

Why apparent? Because it is not real motion , but the one we perceive.

Ex1 : a ball bouncing in a dark room , what do we perceive? Nothing

Ex2 : The barber pole


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{40.PNG}
  \end{subfigure}
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=1\textwidth]{41.PNG}
     \end{subfigure}
\end{figure}

\textbf{Gradient-based Derivation}

It is based on the image brightness constancy equation : from one frame to the next image appearance does not change.
We are basing our analysis based on estimating derivatives. For every x,y pixel acquired at time t ,there will be at time t+1 something identical but in a different position.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{42.PNG}
  \end{subfigure}
\end{figure}

The underlined parts are the partial derivatives and our objective is to estimate u and v.

The optical flow is a vector field subject to the constraint above.

This constraint gives us 1 equation per each image point.

\subsubsection{Optical Flows Algorithms : Lucas Kanade Algorithm}

Many algorithms start from the idea of adding constrints to the undetermined system obtained by the brightness constancy equation,

The assumption of this algorithm is : \textbf{u is constant in a small neighbourhood of a point} , the neighbour points are moving in the same way , so with the same vector u.

This assumption allows us to obtain a system of equations with one equation for each point in the neighbourhood.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{43.PNG}
  \end{subfigure}
\end{figure}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{44.PNG}
  \end{subfigure}
\end{figure}

\vspace{70mm}

\subsection{Correspondence over time : Feature Tracking}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{45.PNG}
  \end{subfigure}
\end{figure}


\subsubsection{The role of Kalman Filter}

Kalman filter allows us to smooth the noisy trajectories
Further, it can be used to fill trajectory gaps: we may use the estimated state as
a hypothesis of the missing measurement
The Pt state covariance matrix provides us with a measure of the state
uncertainty ellipse 

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{46.PNG}
  \end{subfigure}
\end{figure}

When we are tracking multiple corners , we often face association problems.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{47.PNG}
  \end{subfigure}
\end{figure}

\section{Principles of 3D computer vision}

\subsection{Active 3D Sensors}

Time-of-Flight active sensors to measure distance : LIDAR (laser based , short to medium range) , RADAR(RF based , medium to long range).

\subsubsection{The Geometry of image formation}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{48.PNG}
  \end{subfigure}
\end{figure}

When i take a photo what is happening is easy , i'm taking points from 3D world and projective on the image plane 2D.

The focal length is the distance between the camera center and the sensor.

\subsubsection{Scale Ambiguity}

3D information is lost during the projection to the image plane. 

It is not possible (without a proper language ) to distinguish small objects from far objects.

\vspace{50mm}

\subsection{Stereopsis}

Stereo vision refers to the ability to infer information on the 3D structure and distance of a scene from two or more images taken different viewpoints.

\textbf{The correspondence problem} : which parts of the left and right images are projections of the same scene element?

\textbf{The reconstruction problem} : Given a number of corresponding parts of the left and right image , and possibly information on the geometry of the stereo system , what can we say about the 3D location and structure of the observed objects?



\subsubsection{A simple stereo system}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=1\linewidth]{49.PNG}
  \end{subfigure}
\end{figure}

The diagram on the left shows the top view of a stereo system composed of two pinhole cameras. The left and right image planes are coplanar , represented by the segments $I_l$ and $I_r$, $O_l$ and $O_r$ are the centers of the projection. the optical axes are parallel , for this reason the fixation point , defined as the point of intersection of the optical axes, lies infinitely far from the camera. The way in which stereo determines the position in space of P and Q is triangulation.

Let us now assume that the correspondence problem has been solved , and turn to reconstruction.

It is instructive to write the equation underlying the triangulation .

We concentrate on the recovery of the position of a single point P , from its projections $p_l$ and $p_r$.

The distance T , between the centers of projection $O_l$ and $O_r$ , is called baseline of the stereo system.

Let $x_l$ and $x_r$ be the coordinates of $p_l$ and $p_r$ , wrt to principal points $c_l$ and $c_r$ , f the common focal length , and Z the distance between P and the baseline.

From the similar triangles ($p_l$, P , $P_r$) and ($O_l$, P , $O_r$) we have :

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{50.PNG}
  \end{subfigure}
\end{figure}

Whre $d = x_r - x_l$ the disparity , measures the difference in retinal position between the corresponding points in the two images.

From the last formula we can see that depth is inversely proportional to disparity (distant objects seem to move more slowly than close ones).

\subsubsection{The problems of stereopsis}

Stereo covers two main problems : finding correspondences between image pairs (p , p') and reconstructing the 3D position of a point P given its corresponding projections on the images

\subsection{Disparity}

The correspondence problem involves two decisions :

\begin{enumerate}
    \item Which image element to match
    \item Which similarity measure to adopt
\end{enumerate}

The first problem can be solve in two ways : use all pixels in the image ( obtaining dense correspondences or disparity maps) or use subsets of pixels meeting some requirements(obtaining sparse correspondences)


\subsubsection{Dense correspondences}

We consider the so calle correlation methods , the elements to match are image windows of fixed size , and the similarity criterion is a measure of the correlation between windows in the two images. The corresponding element is given by the window that maximizes the similarity criterion within a search region.

We assum we have two rectified images , where conjugate points lie on corresponding scanlines of the image("rows").


Our goal is to obtain a disparity map giving the relative displacement for each pixel.

Assuming a fixation point at infinity , disparity is proportional to the inverse of the distance.

In a standard color coding bright areas correspond to high disparities.

In the next there is a summary of the algorithm

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{51.PNG}
  \end{subfigure}
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=1\textwidth]{52.PNG}
     \end{subfigure}
\end{figure}


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{53.PNG}
  \end{subfigure}
\end{figure}

\subsection{3D Reconstruction}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{54.PNG}
  \end{subfigure}
\end{figure}

\subsubsection{Parameters of a stereo system}

The \textbf{intrisic} parameters characterize the transformation mapping an image point point from camera to pixel coordinates in each camera.

The \textbf{extrinsic} parameters describe the relative position and orientation of the two cameras

The intrinsic parameters, comprehend a minimal set for each camera that include  the coordinates of the principal point and the focal length in pixel.

The extrinsic parameters , instead , describe the rigid transformation (translation and rotation) that brings the reference of the two cameras onto each other.


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=1\linewidth]{55.PNG}
  \end{subfigure}
\end{figure}

\subsubsection{How to relate points in the world with pixels in the image}

The reference frames of the left and right cameras are related via the extrinsic parameters. These define a rigid transformation in 3D space, defined by a translation vector $T = (O_r - O_l)$ and a rotation matrix R. Given a point P in space, the relation between $P_l$ and $P_r$ , is therefore $P_r = R(P_l -T)$

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{56.PNG}
  \end{subfigure}
\end{figure}

Internal parameters can be known a priori or estimated by camera calibration procedures.

External parameters can be obtained starting from point correspondences.

\subsubsection{2D to 3D : points triangualation}

Assuming we know the internal and external parameters.

Given a pair of corresponding in 2D ( metric coordinates on a common reference frame) ($p_l , p_r$).

Estimate the equation of the two projection rays l , l'.

Compute the intersection it will be the 3D reconstruction P

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{57.PNG}
  \end{subfigure}
\end{figure}

\subsection{Quick detour on geometry background}

\subsubsection{Homogeneous Coordinates}

\textbf{Points on a plane} : (x,y) $\in R^2$

\textbf{Lines} -: Equation of a line on a plane : ax + by + c = 0

We may also represent it as $(a,b,c)^T$

Notice that $(ka,kb,kc)^T$ . $k \neq 0$ it is the same line (they belong to the same equivalence class).

This equivalence class is called a \textbf{homogeneous vector}

The line is fixed with the coefficient a,b,c. If i multiply the coefficients with a scalar i got the same line as before.

\textbf{Projective plane} : The projective plane is formed by any $(a,b,c)^T$ representative of an equivalence class : $R^3 - (0,0,0)$

The triplet (0,0,0) could never be a line on the plane.

\textbf{A point and a line} : A point $(x,y)^T$ lies on a line $l = (a,b,c)^T$ IFF ax+by+c = 0

If we define $x = (x,y,1)^T$ then $x^Tl=0$

A point on the Euclidean plane may be represented as a point in the projective plane ( as a 3D vector with a final 1).


\vspace{40mm}

\subsection{Perspective model}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{58.PNG}
  \end{subfigure}
\end{figure}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{59.PNG}
  \end{subfigure}
\end{figure}

\vspace{80mm}

\subsection{Epipolar Geometry}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{60.PNG}
  \end{subfigure}
\end{figure}

Given a stereo pair of cameras , any point in 3D space . defines a plane $\pi_p$, going through P and the centers of projection of the two cameras. The plane $\pi_p$ is called \textbf{epipolar plane}, and the lines where $\pi_p$ intersects te image planes conjugated epipolar planes. The image in one camera of the projection center of the other is called \textbf{epipole}.

The figure shows two pinhole cameras, their projection centers $O_l , O_r$ and image planes $\pi_l . \pi_r$

The vectors $P_l = [X_l,Y_l,Z_l]^T$ and $P_r = [X_r,Y_r,Z_r]^T$ refer to the same 3D point , P thought as a vector in the left and right cameras reference frames respectively. 

The vectors $p_l = [x_l,y_l,z_l]^T$ and $p_l = [x_l,y_l,z_l]^T$ refers to the projections of P onto the left and right image plane respectively , and are expressed in the corresponding reference frame.

Let x and x' two point that intersect the two reference frame.

How is x' constrained?

x' lies on the line $P_r$ intersection between the epipolar plane and the right image plane

$P_r$ is the projection on the image plane of the ray passing through $O_l$ and x.

In practice , when we look for the corresponding point x' we can limit our search to a line , $P_l$.

From a 2D to a 1D problem!

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{61.PNG}
  \end{subfigure}
\end{figure}

The obvious question at this point is , can we estimate the epipolar geometry? How do we determine the mapping between points in one image and epipolar lines in the other?

\textbf{The Essential Matrix E}

The equation of the epipolar plane through P can be written as the coplanarity condition of the vectors $P_l,T,P_l - T$ or $(P_l - T)^T T x P_l = 0$.

Using  $P_r = R(P_l -T)$ we obtain $(R^T P_r)^T x P_l = 0$.

Recalling that a vector product can be written as a multiplication by a rank-deficient matrix , we can write $T x P_l = SP_l$

Then we got , $P_r^TEP_l = 0$ with E = RS.

By construction , S has always rank 2. The matrix E is called the Essential matrix and establishes a natural link between the epipolar constraint and the extrinsic parameters of the stereo system.

\textbf{The Fundamental Matrix}

We now show that the mapping between points and epipolar lines can be obtained from corresponding points only , with no prior information on the stereo system

We can now derive an equivalent equation relating points in pixel coordinates.

$x_m = K_l^-1x$ , $x'_m = K_r^-1x'$.

Then $x'^TK_r^TEK_l^-1x = 0 $

The fundamental matrix satisfies the equation $x'^TFx = 0$, for each pair of (x,x') of corresponding points in pixel coordinates.

Suppose we have two images acquired by cameras with no coinciding centres : $x'^TFx = 0$.

The the fundamental matrix F is the unique 3x3 rank 2 matrix so that, for each corresponding pair (x,x')

Fundamental matrix is a map between point and lines.

$l'= Fx$


\vspace{60mm}

\section{Image Representations}

\subsection{Image and Semantics}

\textbf{Image Classification} : Associate a label to an image.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{62.PNG}
  \end{subfigure}
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=1\textwidth]{63.PNG}
     \end{subfigure}
\end{figure}

X is p-dimensional input vector and Y is the output.

\textbf{Multiclass classification} : The set of possible labels grows

\subsection{Object recognition vs Instance recognition}

Notice the difference. Instance recognition : "his car" , "that bicycle" , "my mug"

Category recognition : generic object recognition , "cars" 

\subsection{Object detection in its classicial formulation}

Object detection is in essence a classification problem. Image regions of variable size are classified : is it an instance of the object or not?

A special case occurs when we have only a class of interest.

Unbalanced classes : In this 380x220 px image we perform millions of tests and we should find only 11 positives.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{64.PNG}
  \end{subfigure}
\end{figure}

\subsection{Global Image Representations}

A list incorporating what we have seen so far : 

\begin{itemize}
    \item Pixels (not so robust)
    \item Graylevels/color/gradient histograms(not too descriptive)
    \item Local keypoints (in Bag of keypoints)
\end{itemize}

\subsection{Finding a global feature vector from local keypoints}

\subsubsection{Bag of words : the inspiration comes from text analysis}

One of the simplest algorithm for category recognition is the bag of words.

This algorithm simply computes the distribution of visual words found in the query image and compares the distribution to those found in the training images.
In Bags of words there are no geometric relationships between parts or features.  

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{65.PNG}
  \end{subfigure}
\end{figure}

\begin{enumerate}
    \item Build a visual dictionary from data
    \item Represent all images with terms frequencies , or more specifically by quantizing its key points with respect to the dictionary
\end{enumerate}

\vspace{100mm}

\subsubsection{BOW - Visual Dictionary}

\textbf{Take 1 - Sparse}

Start considering an appropriate training set of images G.

\begin{itemize}
    \item Extract local features or keypoints in each image
    \item Compute descriptors (SIFT) from each key point and gather all of them in a common set V
    \item Group coherent descriptors in k groups , for instance with K-means
    \item Compute visual words (eg K-means centroids) and collect them in a dictionary D of size k
\end{itemize}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[width=1\linewidth]{66.PNG}
  \end{subfigure}
\end{figure}

\textbf{Take 2 - Dense}

Compute SIFT on a dense set of image patches.

Start considering an appropriate training set of images G

\begin{enumerate}
    \item Compute descriptors from each patch and gather all of them in a common set V
    \item Group coherent descriptors in k groups , for instance with K-means
    \item Compute visual words and collect them in a dictionary D of size K
\end{enumerate}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[width=1\linewidth]{67.PNG}
  \end{subfigure}
\end{figure}

Compute a histogram of the frequencies of all visual words appearing in it.

Depending on the type of images the sparse approach can produce very sparse feature vectors..

For this dense approach can be a good alternative.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=1\linewidth]{68.PNG}
  \end{subfigure}
\end{figure}

\subsubsection{Coding-Pooling Image representation}


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=1\linewidth]{69.PNG}
  \end{subfigure}
\end{figure}

In the BoW case , given a dictionary D of size $|D| = D$

Each feature x is encoded to a D-dimensional vector : 

$\phi_BOW (x) = [0,...,0,1,0,...0]^T$ , the non-zero position is determined based on a nearest neighbor assignment rule.

The pooling step represents the histogram computation.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=1\linewidth]{70.PNG}
  \end{subfigure}
\end{figure}

\vspace{40mm}

\subsubsection{Pooling over a spatial pyramid}

Goal : Restore some locality (we loose invariance)

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{71.PNG}
  \end{subfigure}
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=1\textwidth]{72.PNG}
     \end{subfigure}
\end{figure}

\vspace{50mm}

\section{Object Detection : The Classical Way}

\subsection{Problem Definition}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=1\linewidth]{73.PNG}
  \end{subfigure}
\end{figure}

\subsubsection{Definitions : Object Detection}

Object detection is in essence a combination of binary classification problems. Image regions of variable size are classified. 

Classification : associate a global label to the image (kitten or not , the first image is not an image of the kittens, because there are multiple kittens and contains other stuff).

Detection : It is the association of a label to a portion of the image.

With unbalanced class , the second image is 380 x 220 px image we perform 6.4 x 10 to 5 tests and we should find only 11 positives.

The training set contains  (third image) , images of positive examples and negative examples (fourth images).

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.2\linewidth}
    \includegraphics[width=1\linewidth]{74.PNG}
  \end{subfigure}
\end{figure}

\subsubsection{Object detection : Basic idea}

Slide a window across image and evaluate an object model at every location.



\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{75.PNG}
  \end{subfigure}
\end{figure}


I'll have multiple square that will say that inside that square there is a face , this is a requirement. Because we cannot have only one yes.

In the past object detection has usually been adopted for very "meaningful" and very specific classes (faces , people , cars).

We have to remember that humans can recognize low-resolution faces of familiar people.

\subsubsection{Object Detection and Data Representation}

The complexity of the task requires we find appropriate data representations.

Before the DL era , most of the methods started off from local elements and later derived intermediate and then global information.

Local information may be :

\begin{itemize}
    \item Encoded by means of pre-defined general purpose dictionaries
    \item Learnt from data
\end{itemize}

Many effective object detection methods resort to over-complete general purpose sets of features as they are effective for modelling "standardized" visual information. 

Today we explore two alternatives , building on methods we know of : face detection with wavelet-based features and Pedestrian detection with gradient-based features.

\subsection{Face Detection}

\subsubsection{2D Wavelets}


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{76.PNG}
  \end{subfigure}
\end{figure}


\subsubsection{2d Haar Basis}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{77.PNG}
  \end{subfigure}
\end{figure}

Haar features or rectangle features : +1 and -1 filter coefficients .

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{78.PNG}
  \end{subfigure}
\end{figure}

Feature value is sum of the pixels in the white region minus the sum of the pixels in the black region.

Value = $\sum$ (pixels in the white area) - $\sum (pixels in black area)$

Rectangle features can be computed very rapidly using an intermediate representation for the image which we call the \textbf{Integral Image}. 

The integral image at location x,y contains the sum of the pixels above and to the left of x,y ,  inclusive :

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=1\linewidth]{79.PNG}
  \end{subfigure}
\end{figure}

where ii(x,y) is the integral image and i(x,y) is the original 

Using the integral image any rectangular sum can be computed in four array references.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{80.PNG}
  \end{subfigure}
\end{figure}

We have a lots of features , at least with integral images we may learn to compute them faster but they are still many (lots of redundancy)

\subsubsection{Features Map}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{81.PNG}
  \end{subfigure}
\end{figure}

We are going to generate , for every image , a set based on those pattern.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{82.PNG}
  \end{subfigure}
\end{figure}

Large amount of features, these features are also very redundant and correlated (because of the properties of images)... maybe we can just compute a smaller subset.


\subsubsection{Feature Selection Procedures}

Set up a learning procedure to select only a subset of meaningful features. These features will be the only one computed at run time.

Feature selection methods for face detection we will discuss : Regularization with a sparsity penalty and a reference face detector (Viola and Jones).


\subsection{Viola and Jones face Detector}

Effective and Efficient. 

The key elements of this approach are : Haar features , embedded variable selection with a boosting procedure and optimization with a classifiers cascade.

\subsubsection{Boosting (overview)}

Boosting is a classification scheme that works by combining weak learners into a more accurate strong ensemble classifier (a weak learner needs only do better than chance).

Training consists of multiple boosting rounds.

During each boosting round, we select a weak learner that does well on examples that were hard for the previous weak learners.

"Hardness" is captured by weights attached to training examples



\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{83.PNG}
  \end{subfigure}
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=1\textwidth]{84.PNG}
     \end{subfigure}
\end{figure}


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{85.PNG}
  \end{subfigure}
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=1\textwidth]{86.PNG}
     \end{subfigure}
\end{figure}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{87.PNG}
  \end{subfigure}
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=1\textwidth]{88.PNG}
     \end{subfigure}
\end{figure}

\subsubsection{VJ : ADABOOST and Haar Features}

The weak learning algorithm is designed to select the single rectangle features which best separate the positive and negative examples. For each feature , the weak learner determines the optimal threshold classification function , such that the minimum number of examples are misclassified.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{89.PNG}
  \end{subfigure}
\end{figure}

For each round of boosting :

\begin{itemize}
    \item Evaluate each rectangle filter on each example
    \item Select best threshold for each filter
    \item Select best filter and threshold combination as the weak learner
    \item Reweight examples
\end{itemize}


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=1\linewidth]{90.PNG}
  \end{subfigure}
\end{figure}

\vspace{50mm}

\subsubsection{VJ : Classifiers Cascade}

Start with simple classifiers which reject many of the negative subwindows while detecting almost all positive sub-windows

Positive response from the first classifier triggers the evaluation of a second classifier , and so on.

A negative outcome at any point leads to the immediate rejection of the sub-window.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=1\linewidth]{91.PNG}
  \end{subfigure}
\end{figure}

Solves several problems : improves speed by early rejection of non-face regions by simple classifiers, reduces false positive rates.

The detection rate and the false positive rate of the cascade are found by multiplying the respective rates of the individual stages.

Training the cascade : 

\begin{itemize}
    \item Set target detection and false positive rates for each stage.
    \item Keep adding features to the current stage until its target rates have met.
    \item Test on a validation set : if the overall false positive rate is not low enough , then add another stage.
    \item The classifiers in the cascade are trained using AdaBoost on the remaining set of example images
    \item Thus, if the first stage classifier rejects a number of images then these images are not included when training the second stage classifier.
    \item But use false positive from current stage as the negative training examples for the next stage
\end{itemize}

The sub-images which do not contain the object are gradually removed from consideration leaving just the objects which are sought.


\subsection{Pedestrian Detection}

Another example of object detection.

Notice that the underlying problem is quite different.

We could set up the same procedure described for face detection.

Among the better known methods : Histograms of Oriented Gradients (HoGs)

\subsubsection{Hog - Computational sketch}

Given an image region of appropriate aspect ration , we have to compute the image gradient.

\textbf{Cell histogram} : each pixel in a cell casts a weighted vote for an orientation (quantized).

\textbf{Blocks} : Histograms in a block are concatenated and then normalized.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{92.PNG}
  \end{subfigure}
\end{figure}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{93.PNG}
  \end{subfigure}
\end{figure}

\subsection{Conclusions}

Object detection can be seen as a combination of binary image classification tasks (object vs non objects).

It has been long addressed for very specific classes (faces, pedestrians, cars).

Data representation has been the main focus of research, together with computational efficiency.

\section{Object Detection and Deep Learning}

Given a class of objects of interest (face , pedestrian) and given an input image I which contains N instances of the object , locate all the instances that is , find ${(x_i,y_i,w_i,h_i)}$; i = 1..N the center and dimensions of boxes that best localize the objects in image I.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{94.PNG}
  \end{subfigure}
\end{figure}

\subsection{Detection evaluation}

\subsubsection{How are object detectors evaluated}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \includegraphics[width=1\linewidth]{95.PNG}
  \end{subfigure}
     \begin{subfigure}[b]{0.39\textwidth}
         \centering
         \includegraphics[width=1\textwidth]{96.PNG}
     \end{subfigure}
\end{figure}

\subsection{Problem (re)definition}

Given a set of classes of interest C ( e.g. Street objects , office objects) and given an input image I that contains N object instances : find the center , the class and the dimensions of boxe (aligned with the coordinate system) that best localize the objects in image I , formally : ${(c_i,x_i,y_i,w_i,h_i)}; c_i \in C, i = 1..N$

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=1\linewidth]{97.PNG}
  \end{subfigure}
\end{figure}

\subsubsection{Sliding Window}

Apply a CNN to many different parts of the image , CNN classifies each crop as object or background.

The problem is that it needs to apply CNN to huge number of locations and scales and so it's very compuationally expensive.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=1\linewidth]{98.PNG}
  \end{subfigure}
\end{figure}


\subsubsection{Classification + Localization}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=1\linewidth]{99.PNG}
  \end{subfigure}
\end{figure}

\vspace{10mm}

\subsection{Region-based CNNs}

\subsubsection{Region proposals}

Find image regions that are likely to contain objects.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{100.PNG}
  \end{subfigure}
\end{figure}

\subsubsection{R-CNN}

Two steps : 

\begin{enumerate}
    \item Selective search : identification of region proposals
    \item  CNN
\end{enumerate}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{101.PNG}
  \end{subfigure}
\end{figure}


\textbf{"Slow R-CNN"}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{102.PNG}
  \end{subfigure}
\end{figure}

\vspace{40mm}

\textbf{"Fast" R-CNN}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{103.PNG}
  \end{subfigure}
\end{figure}

\subsubsection{Faster R-CNN}

The network learns its proposals : insert a \textbf{Region Proposal Network (RPN)} to predict proposals from features

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{104.PNG}
  \end{subfigure}
\end{figure}

A convolutional backbone is used to extract features from the input image.

A RPN is used to generate proposals ( regions with high probability of containing object. The network follows a brute-force approach to classify regions of the image to one of the two classes : background/object).

These proposals are used to perform RoI pooling of the feature map and extract a region-based features.

These features are then used to predict a class probability and a box offset.

\subsubsection{Region Proposal Network}

Red and green line forms the anchor box of fixed size centered on each point of the feature map.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=1\linewidth]{105.PNG}
  \end{subfigure}
\end{figure}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=1\linewidth]{106.PNG}
  \end{subfigure}
\end{figure}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=1\linewidth]{107.PNG}
  \end{subfigure}
\end{figure}

\section{Image Segmentation}


Segmentation is a difficult task to solve and to formulate. 

Images can also be divided in somewhat uniform image regions.

Uniformity is based on a quality (e.g. brightness , color or motion).

\begin{itemize}
    \item Image segmentation (regions that can be further analyzed)
    \item Superpixels computation (features)
\end{itemize}

\subsection{Image Segmentation}

\textbf{Unsupervised segmentation}: is based on pixels and appearance information.

\vspace{5mm}

\textbf{Supervised segmentation}: is guided by some semantic concept

\subsubsection{Unsupervised image segmentation methods overview}

\begin{itemize}
    \item Edge-based methods: look for discontinuities
    \item Region-based methods: look for similarity regions according to some criteria
\end{itemize}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{108.PNG}
  \end{subfigure}
\end{figure}

\subsubsection{Region based methods}

\textbf{Thresholding} ( followed by connected components computation). Region growing. Morphological watersheds.

Clustering methods(K-means,mean shift).

\subsubsection{Image segmentation : definition}

Let R be the spatial region occupied by an image. Image segmentation may defined as the process of partitioning R into n subregions.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{109.PNG}
  \end{subfigure}
\end{figure}

We already know a thresholding mehtod which is : \textbf{motion segmentation)}.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{110.PNG}
  \end{subfigure}
\end{figure}

How to choose T automatically????

\begin{enumerate}
    \item Select an initial estimate of T
    \item Segment the image using T (G1 are pixels so that f(x,y) > T, G2 the others)
    \item Compute the mean intensity values in G1 and G2, let them be m1 and m2
    \item Compute a new threshold : $T = 1/2 (m_1 + m_2)$
    \item Repeat steps 2 to 4 until T does not change any more
\end{enumerate}

\subsubsection{Otsu's method for global thresholding}

Objective : Segment foreground/background by maximizing the between class variance

All computations are performed on the normalized histogram of the image (the value of each bin is pi).

Let us consider a threshold $T(k) 0 < k < L-1$ (L is the maximum value in the range, eg 255) and use it to segment image pixels in two classes : C1 pixels in the range [0,k] and C2 pixels in the range [k+1,L-1].


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{111.PNG}
  \end{subfigure}
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=1\textwidth]{112.PNG}
     \end{subfigure}
\end{figure}


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{113.PNG}
  \end{subfigure}
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=1\textwidth]{114.PNG}
     \end{subfigure}
\end{figure}

\vspace{10mm}

\subsubsection{Variable Thresholding}

Objective: choosing different thresholds for different image parts

Simple idea: subdivide an image into non overlapping rectangles.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{115.PNG}
  \end{subfigure}
\end{figure}

\textbf{Variable Thresholding from local image properties}

\vspace{5mm}

Compute a threshold $T_{xy}$ in each point (x,y) based on its neighbourhood properties.

A simple example of this approach is the following : $T_{xy} = a \sigma_{xy} + b m_{xy}$    $a,b > 0$

\vspace{1mm}

\textbf{Variable Thresholding by moving average}

\vspace{5mm}

This approach has been widely adopted for text segmentation.

There is a uniform distribution of text and background over space.

\subsection{Color Segmentation}

\subsubsection{Color-based Thresholding : one possibility}

\begin{enumerate}
    \item Smooth the image
    \item Transform to HSV space
    \item Discard V
    \item Choose thresholds for H and S appropriate for the task you are trying to address.
\end{enumerate}

\textbf{Examples}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{116.PNG}
  \end{subfigure}
\end{figure}

\vspace{80mm}

\subsection{Clustering Methods}

What if you don't have a specific color in mind?

For more generic color segmentation: image pixels can be grouped in clusters of elements with similar colors.

A classical choice is K-means, you already met several times.

\subsubsection{K-means clustering reminder}

Given a set of data $(x_1,...,x_n)$ the goal is to partition it into K sets $S = (S_1,...,S_K)$ so to minimize the within-cluster variance :

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.7\linewidth}
    \includegraphics[width=1\linewidth]{117.PNG}
  \end{subfigure}
\end{figure}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{118.PNG}
  \end{subfigure}
\end{figure}

\textbf{The challenge is : how to choose K}

Try multiple K and search the ones that give the highest confidence by using a cluster quality indicator.

This approach does not take into account the cluster size.

Pros: simplicity mainly

Cons: if we do not have priors on the choice of K we may obtain non deterministic results. The method does not prevent the formation of unbalanced clusters.

\vspace{20mm}

\subsection{Graph-based Image Segmentation}

The goal of this approach is to devise a segmentation method that "extracts a global impression of an image".

The segmentation process here becomes a graph partitioning problem.

This involved the following questions : what is a precise criterion for a good partition? How can we make it computationally efficient?

\subsection{Superpixels computation}

Small image parts characterized by internal uniformity.

Superpixels should be evenly distributed and should follow very well objects boundaries.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{119.PNG}
  \end{subfigure}
\end{figure}

SLIC (Simple linear iterative clustering)

Simple and fast algorithm , based on Kmeans,

Search space limited to a region proportional to the desired super pixel size.

A weighted distance measure combines color and spatial proximity.

\section{Instance Segmentation}

\subsection{Semantic Segmentation}

\subsubsection{A typical CNN layer}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{120.PNG}
  \end{subfigure}
\end{figure}

Semantic segmentation labels each pixel in the image with a category labels. Do not differentiate instances , only care about pixels.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{121.png}
  \end{subfigure}
\end{figure}

\vspace{10mm}

We can do it with sliding window technique

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=1\linewidth]{122.PNG}
  \end{subfigure}
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=1\textwidth]{123.PNG}
     \end{subfigure}
\end{figure}

\end{document}
